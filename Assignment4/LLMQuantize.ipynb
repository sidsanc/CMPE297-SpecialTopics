{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPBqnCabs0ofRDAK6vWYQ4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sidsanc/CMPE297-SpecialTopics/blob/main/Assignment4/LLMQuantize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAezw2GRHJZt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eMFPiPfbVVC"
      },
      "source": [
        "Reference:\n",
        "\n",
        "https://github.com/ggerganov/llama.cpp\n",
        "\n",
        "https://colab.research.google.com/github/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_chat_module_getting_started.ipynb#scrollTo=TNmg9N_NDmYF"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantization"
      ],
      "metadata": {
        "id": "UmDO4IF6wxCP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNx-y7ZNbIAP",
        "outputId": "55d4fc37-b8f1-4837-a02a-47471bf62ebc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/302.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m204.8/302.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.8.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.7.22)\n",
            "Installing collected packages: huggingface_hub\n",
            "Successfully installed huggingface_hub-0.18.0\n",
            "Collecting sentencepiece==0.1.98\n",
            "  Downloading sentencepiece-0.1.98-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.98\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub\n",
        "!pip install sentencepiece==0.1.98\n",
        "!pip install gguf>=0.1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_S4ctPYnzCs"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import snapshot_download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419,
          "referenced_widgets": [
            "9f857b910b89434bba3d0702d72c662c",
            "4b3d15e6a0344163b7699183546c361f",
            "8e3cba49eb6a401cb07a9b3582d1ec84",
            "76585d103efa4f4fa698aaf17f2e999c",
            "dadcbc0932ef495bb20486c8512f6b31",
            "5848708bc89347988929c923026fe845",
            "133b3a4a52a94a50bc25c161ae775b5d",
            "adef6b17b5df4ef19eab9757fefc576e",
            "17e130aa71e941b8a6897bf4a9e1a943",
            "3f4838417875400c962513cde0ad49ce",
            "0e931e07776d4cd19fce1d30e6bb33b1",
            "6be78db6d2f345f69b80f9d2656ce676",
            "ee14b5a2f18c45f29cad2ff50323e56f",
            "f5feed8fe0a04f1cb4bc3e2c6bcc44b2",
            "d6fa7b57d79441fa81ce290cc0b3b86a",
            "bfeee6c84b4047578b16bb154fa47de0",
            "ca288d1ba2a34a6b86b23ce5b77858f8",
            "0c386091c80b4fefb19d3ffde0b8ced1",
            "6ef65dd0a8cd4799ae51688fb3f1342e",
            "0c23d30c40e94d5c988a3fa370170cd8",
            "377f06cdacde4bc29dc93f42c16b38c8",
            "d9768bebd0ae471c9fea6bcf1e58de5d",
            "50f21292b7b64de2b672343b9c657697",
            "4bf3d7d33b0b41b8b3c999fb0f2f39ff",
            "556be46d11454117ae598051d0b73c3c",
            "dc5783c18b614256baeedf0b03069651",
            "a1a0cde4e8d0408eadbd8a27c959f553",
            "046df78c5b4043a59184c03360a9cb37",
            "c91283f8cded478bb88b4d5d2c479f32",
            "9944ed752ea34b68a913c8f34a831172",
            "55bcc3533f9f4753bc33d46524527d7f",
            "ecaca83031fb4a39b3a3d7e02188b6f9",
            "fa5353846d0a454d8f8e7cddcc42582c",
            "26134a540bc9406da7d02b0b39f17760",
            "c278b2c64179497e950088844fa321b9",
            "8183c06a940e421082791772922eb151",
            "245549f133c0478a8b1118b2c0255f09",
            "d31c795e35884878a411301017a2a10c",
            "a8417f0c411f44b3b4360b879a8dca6d",
            "14c2b9662c2b45449124bed94119d52f",
            "c92e8c52c9fa44ccb275281ab477e3f4",
            "a1e168efd4c944389b895b7e6733c72c",
            "8ba60e2e955046f58a81696343a7c2f0",
            "bce8334d129a48a8a9b7249d409ad9bc",
            "4918280ab56b4fa58552cacd1cdaf42c",
            "39fcd43820e04013959dd8ecebcf0333",
            "a60db3b7004942b88655668aad382c96",
            "242ccafdc8194c1b913fdd3d313d4422",
            "6949d46a3cf14d1aa6b595c22d90be03",
            "3d9df6fc119845e5b137de7fba851272",
            "cc939d90401b424ea3f68af9dcef7c5e",
            "0f265bd2260b4dd4b975444467226a2a",
            "8006c2c89d24493f83edc05376ced408",
            "e936bfbc6b274f70bde8247e7bb1e68c",
            "d941039b53d5420aa1ae2d9640d43db6",
            "57be6d20015b4b3d89751c759f092cca",
            "23b86078141046cba688a8856afb4d3b",
            "04b8555faeba45989b072f1ef1b5a800",
            "9fb9517cb43743cf918c2496b14a9fb7",
            "fc47c4af17994fd9af57c6a5fbc39a5a",
            "84c3e8960e5947819ffaa47338ecd133",
            "eeef43dcca354d548c5f1d0f704b59d6",
            "bddf9b288d2a435a88f6211211356401",
            "4fafc979d2004e709e983295a1ee2cf6",
            "a17ec772012d4029a6d7aa6a03b15912",
            "e9d12b7034fe455299d0d1c9f5202325",
            "6691fb1de79a421ab619a959c1a8960d",
            "b2acf5ad093d4ac3b3b682047b6d8e55",
            "1aa9a2a662714ef7a74de6842363002c",
            "5de453c3b0f4421e957ccf79e4a76714",
            "b9dfa9c16e1141e8b0916ddede37086d",
            "7063cc0968134b99b564ff4e6eb33c0d",
            "d797830c570449489e1084ae4f1f5642",
            "a86e33589295412fa03760856c89052c",
            "04daec2cd9784e71827e7ad3586c6472",
            "a91b31d2a72245a7b93e771d25d94531",
            "2c9cc172470a44bd87c38c8e35bec4a7",
            "99a3073cec7e4ffea28bc3c5c99ad315",
            "95ec778fad4f423497f5e4b9aff469d0",
            "8a226725ab674198a3bf80c6d19fc1ca",
            "dec2701bcd3142638e8afde07bd4ea2d",
            "bbd52314eefa4e45b669aace1e26a9e7",
            "ef1120db3c6b4419b0010a7d9c3d9560",
            "1e8926dc2a27419cad7c95c1983fd80d",
            "14c5dc82a277441c82f423729c04b7b5",
            "c245341cc3824530ab5cfa592347a3fe",
            "a75c611eb6e64aa39dd5942a077f286e",
            "b6401a0b896a43d98c4e11cd83d6ce57",
            "e82c5de166194ae3bb4f14e70e44465a",
            "c711c1a92fa1407ba5c536fc0288b6ff",
            "ff3792a25c5e4f69ad90cbf9bd62e2ea",
            "eef68acd80254785845deab7ee27cc16",
            "7757755e32ba4bb6bc2303a5ded11efe",
            "ebad30f58bd04044890e48618431cba9",
            "796dde19378b4204ae9935dd55e7bf73",
            "3395f76ef22744d8b0e1e7eea4c6ebd9",
            "16debb7d612945d3b69df78a70bd0015",
            "727c825148cf4a06a6c738ef56f42765",
            "99bb7a70cf1e498ba5e97235b7b84e35",
            "2a7bfc2d52a24f738847a949cf290b45",
            "60aa38a4a7d14d90bc8b52855d5a8fd6",
            "a85ca9c449a04d8fb53c6c24ac8267b7",
            "e6f095f25c8c466290c52cad16058842",
            "cc84482e8f26448fa8d2343ab8454317",
            "ae023987d1e54c35848f053e88e30b03",
            "dbbb5238cc1e4cc39210f7d4a3c3e492",
            "9f63665d6502468dbd3382e5745a5138",
            "a94af21cbae741bbaea73f5fdb711aaf",
            "e5235f9066c7445b8838042cc3b4196a",
            "121e9adc4da0461fb88952660ca9ae03",
            "a9809e1a86a9430ebbd79e3e3ed25ab4",
            "0231af98a9e34e479dbf32c73fb27b5a",
            "b003f585038d4cb3b38a2f12a66cef12",
            "a91adea339d040d3b34750ca575ebc6a",
            "2a328c655162463cb959b603c413ad7f",
            "75163f86c8e949229473a1e089faa817",
            "a74dcb822bd942dcb6976ac0afc981df",
            "cbc3d973e0ba401bbe1cef453d9e8767",
            "5dc1a927d32045e19bde4a5470f11c52",
            "c5db590cf1a9465e80263009f66d2d0e",
            "464c02ff6b014ef8a73bfd1e0a0b8571",
            "03292221c6a04422b56a5fa6a1108669",
            "2b4c2fc67ecb4d7fbff3ef12b6e546a9",
            "5007f671693b495380e3f796bcb4cc97",
            "6623059615804e99b2044573e4200be5",
            "c8daa9c5900e4a56bb2a9571b0e15269",
            "9402e268fe184f62a0a4a24c158d946b",
            "10bd88e65001487cbdfc8074f1e368e8",
            "52f56b1df8fc42baab200c391f8c53c7",
            "f15985dfcbf741aeb4484d9a6410e090",
            "99945350b1e248039b3494f24f2e94ce",
            "9b218a5e840e410187c4d0f5bdfa04c5"
          ]
        },
        "id": "EsNUVs_fbZeG",
        "outputId": "21077a9c-a419-4b96-9b8f-f1cf04b7a369"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f857b910b89434bba3d0702d72c662c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)0cb978bdf14bcd3a6/generation_config.json:   0%|          | 0.00/192 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6be78db6d2f345f69b80f9d2656ce676"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)33f0c6ea0cb978bdf14bcd3a6/.gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50f21292b7b64de2b672343b9c657697"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)bdf14bcd3a6/pytorch_model.bin.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26134a540bc9406da7d02b0b39f17760"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)2a3a433f0c6ea0cb978bdf14bcd3a6/README.md:   0%|          | 0.00/1.97k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4918280ab56b4fa58552cacd1cdaf42c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00001-of-00003.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57be6d20015b4b3d89751c759f092cca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)3a433f0c6ea0cb978bdf14bcd3a6/config.json:   0%|          | 0.00/638 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6691fb1de79a421ab619a959c1a8960d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)cb978bdf14bcd3a6/special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99a3073cec7e4ffea28bc3c5c99ad315"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)a0cb978bdf14bcd3a6/tokenizer_config.json:   0%|          | 0.00/749 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e82c5de166194ae3bb4f14e70e44465a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00003-of-00003.bin:   0%|          | 0.00/6.18G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a7bfc2d52a24f738847a949cf290b45"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00002-of-00003.bin:   0%|          | 0.00/9.90G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9809e1a86a9430ebbd79e3e3ed25ab4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03292221c6a04422b56a5fa6a1108669"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/vicuna-hf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "model_id=\"lmsys/vicuna-13b-v1.5\"\n",
        "snapshot_download(repo_id=model_id, local_dir=\"vicuna-hf\", local_dir_use_symlinks=False, revision=\"main\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b_y8jCfdXlO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4959bff2-6c4e-42ec-cb8b-b44dc59dbdf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model file /content/vicuna-hf/pytorch_model-00001-of-00003.bin\n",
            "Loading model file /content/vicuna-hf/pytorch_model-00001-of-00003.bin\n",
            "Loading model file /content/vicuna-hf/pytorch_model-00002-of-00003.bin\n",
            "Loading model file /content/vicuna-hf/pytorch_model-00003-of-00003.bin\n",
            "params = Params(n_vocab=32000, n_embd=5120, n_layer=40, n_ctx=4096, n_ff=13824, n_head=40, n_head_kv=40, f_norm_eps=1e-05, f_rope_freq_base=None, f_rope_scale=None, ftype=<GGMLFileType.MostlyQ8_0: 7>, path_model=PosixPath('/content/vicuna-hf'))\n",
            "Loading vocab file '/content/vicuna-hf/tokenizer.model', type 'spm'\n",
            "Permuting layer 0\n",
            "Permuting layer 1\n",
            "Permuting layer 2\n",
            "Permuting layer 3\n",
            "Permuting layer 4\n",
            "Permuting layer 5\n",
            "Permuting layer 6\n",
            "Permuting layer 7\n",
            "Permuting layer 8\n",
            "Permuting layer 9\n",
            "Permuting layer 10\n",
            "Permuting layer 11\n",
            "Permuting layer 12\n",
            "Permuting layer 13\n",
            "Permuting layer 14\n",
            "Permuting layer 15\n",
            "Permuting layer 16\n",
            "Permuting layer 17\n",
            "Permuting layer 18\n",
            "Permuting layer 19\n",
            "Permuting layer 20\n",
            "Permuting layer 21\n",
            "Permuting layer 22\n",
            "Permuting layer 23\n",
            "Permuting layer 24\n",
            "Permuting layer 25\n",
            "Permuting layer 26\n",
            "Permuting layer 27\n",
            "Permuting layer 28\n",
            "Permuting layer 29\n",
            "Permuting layer 30\n",
            "Permuting layer 31\n",
            "Permuting layer 32\n",
            "Permuting layer 33\n",
            "Permuting layer 34\n",
            "Permuting layer 35\n",
            "Permuting layer 36\n",
            "Permuting layer 37\n",
            "Permuting layer 38\n",
            "Permuting layer 39\n",
            "model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [32000, 5120]\n",
            "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F16    | [5120, 5120]\n",
            "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F16    | [5120, 5120]\n",
            "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F16    | [5120, 5120]\n",
            "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F16    | [5120, 5120]\n",
            "skipping tensor blk.0.attn_rot_embd\n",
            "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F16    | [13824, 5120]\n",
            "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F16    | [13824, 5120]\n",
            "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F16    | [5120, 13824]\n",
            "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F16    | [5120]\n",
            "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F16    | [5120]\n",
            "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F16    | [5120, 5120]\n",
            "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F16    | [5120, 5120]\n",
            "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F16    | [5120, 5120]\n",
            "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F16    | [5120, 5120]\n",
            "skipping tensor blk.1.attn_rot_embd\n",
            "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F16    | [13824, 5120]\n",
            "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F16    | [13824, 5120]\n",
            "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F16    | [5120, 13824]\n",
            "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F16    | [5120]\n",
            "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F16    | [5120]\n",
            "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F16    | [5120, 5120]\n",
            "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F16    | [5120, 5120]\n",
            "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F16    | [5120, 5120]\n",
            "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F16    | [5120, 5120]\n",
            "skipping tensor blk.2.attn_rot_embd\n",
            "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F16    | [13824, 5120]\n",
            "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F16    | [13824, 5120]\n",
            "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F16    | [5120, 13824]\n",
            "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F16    | [5120]\n",
            "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F16    | [5120]\n",
            "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F16    | [5120, 5120]\n",
            "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F16    | [5120, 5120]\n",
            "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F16    | [5120, 5120]\n",
            "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F16    | [5120, 5120]\n",
            "skipping tensor blk.3.attn_rot_embd\n",
            "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F16    | [13824, 5120]\n",
            "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F16    | [13824, 5120]\n",
            "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F16    | [5120, 13824]\n",
            "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F16    | [5120]\n",
            "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F16    | [5120]\n",
            "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F16    | [5120, 5120]\n",
            "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F16    | [5120, 5120]\n",
            "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F16    | [5120, 5120]\n",
            "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F16    | [5120, 5120]\n",
            "skipping tensor blk.4.attn_rot_embd\n",
            "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F16    | [13824, 5120]\n",
            "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F16    | [13824, 5120]\n",
            "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F16    | [5120, 13824]\n",
            "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F16    | [5120]\n",
            "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F16    | [5120]\n",
            "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F16    | [5120, 5120]\n",
            "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F16    | [5120, 5120]\n",
            "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F16    | [5120, 5120]\n",
            "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F16    | [5120, 5120]\n",
            "skipping tensor blk.5.attn_rot_embd\n",
            "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F16    | [13824, 5120]\n",
            "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F16    | [13824, 5120]\n",
            "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F16    | [5120, 13824]\n",
            "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F16    | [5120]\n",
            "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F16    | [5120]\n",
            "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F16    | [5120, 5120]\n",
            "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F16    | [5120, 5120]\n",
            "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F16    | [5120, 5120]\n",
            "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F16    | [5120, 5120]\n",
            "skipping tensor blk.6.attn_rot_embd\n",
            "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F16    | [13824, 5120]\n",
            "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F16    | [13824, 5120]\n",
            "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F16    | [5120, 13824]\n",
            "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F16    | [5120]\n",
            "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F16    | [5120]\n",
            "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F16    | [5120, 5120]\n",
            "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F16    | [5120, 5120]\n",
            "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F16    | [5120, 5120]\n",
            "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F16    | [5120, 5120]\n",
            "skipping tensor blk.7.attn_rot_embd\n",
            "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F16    | [13824, 5120]\n",
            "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F16    | [13824, 5120]\n",
            "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F16    | [5120, 13824]\n",
            "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F16    | [5120]\n",
            "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F16    | [5120]\n",
            "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F16    | [5120, 5120]\n",
            "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F16    | [5120, 5120]\n",
            "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F16    | [5120, 5120]\n",
            "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F16    | [5120, 5120]\n",
            "skipping tensor blk.8.attn_rot_embd\n",
            "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F16    | [13824, 5120]\n",
            "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F16    | [13824, 5120]\n",
            "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F16    | [5120, 13824]\n",
            "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F16    | [5120]\n",
            "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F16    | [5120]\n",
            "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F16    | [5120, 5120]\n",
            "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F16    | [5120, 5120]\n",
            "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F16    | [5120, 5120]\n",
            "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F16    | [5120, 5120]\n",
            "skipping tensor blk.9.attn_rot_embd\n",
            "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F16    | [13824, 5120]\n",
            "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F16    | [13824, 5120]\n",
            "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F16    | [5120, 13824]\n",
            "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F16    | [5120]\n",
            "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F16    | [5120]\n",
            "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.10.attn_rot_embd\n",
            "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.11.attn_rot_embd\n",
            "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.12.attn_rot_embd\n",
            "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.13.attn_rot_embd\n",
            "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.14.attn_rot_embd\n",
            "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.15.attn_rot_embd\n",
            "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.16.attn_rot_embd\n",
            "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.17.attn_rot_embd\n",
            "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.18.attn_rot_embd\n",
            "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.19.attn_rot_embd\n",
            "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.20.attn_rot_embd\n",
            "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.21.attn_rot_embd\n",
            "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.22.attn_rot_embd\n",
            "model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.23.attn_rot_embd\n",
            "model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.24.attn_rot_embd\n",
            "model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.25.attn_rot_embd\n",
            "model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.26.attn_rot_embd\n",
            "model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.27.attn_rot_embd\n",
            "model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.28.attn_rot_embd\n",
            "model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.29.attn_rot_embd\n",
            "model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.30.attn_rot_embd\n",
            "model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.31.attn_rot_embd\n",
            "model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.32.self_attn.q_proj.weight          -> blk.32.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.32.self_attn.k_proj.weight          -> blk.32.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.32.self_attn.v_proj.weight          -> blk.32.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.32.self_attn.o_proj.weight          -> blk.32.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.32.attn_rot_embd\n",
            "model.layers.32.mlp.gate_proj.weight             -> blk.32.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.32.mlp.up_proj.weight               -> blk.32.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.32.mlp.down_proj.weight             -> blk.32.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.32.input_layernorm.weight           -> blk.32.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.32.post_attention_layernorm.weight  -> blk.32.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.33.self_attn.q_proj.weight          -> blk.33.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.33.self_attn.k_proj.weight          -> blk.33.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.33.self_attn.v_proj.weight          -> blk.33.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.33.self_attn.o_proj.weight          -> blk.33.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.33.attn_rot_embd\n",
            "model.layers.33.mlp.gate_proj.weight             -> blk.33.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.33.mlp.up_proj.weight               -> blk.33.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.33.mlp.down_proj.weight             -> blk.33.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.33.input_layernorm.weight           -> blk.33.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.33.post_attention_layernorm.weight  -> blk.33.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.34.self_attn.q_proj.weight          -> blk.34.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.34.self_attn.k_proj.weight          -> blk.34.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.34.self_attn.v_proj.weight          -> blk.34.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.34.self_attn.o_proj.weight          -> blk.34.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.34.attn_rot_embd\n",
            "model.layers.34.mlp.gate_proj.weight             -> blk.34.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.34.mlp.up_proj.weight               -> blk.34.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.34.mlp.down_proj.weight             -> blk.34.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.34.input_layernorm.weight           -> blk.34.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.34.post_attention_layernorm.weight  -> blk.34.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.35.self_attn.q_proj.weight          -> blk.35.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.35.self_attn.k_proj.weight          -> blk.35.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.35.self_attn.v_proj.weight          -> blk.35.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.35.self_attn.o_proj.weight          -> blk.35.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.35.attn_rot_embd\n",
            "model.layers.35.mlp.gate_proj.weight             -> blk.35.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.35.mlp.up_proj.weight               -> blk.35.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.35.mlp.down_proj.weight             -> blk.35.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.35.input_layernorm.weight           -> blk.35.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.35.post_attention_layernorm.weight  -> blk.35.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.36.self_attn.q_proj.weight          -> blk.36.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.36.self_attn.k_proj.weight          -> blk.36.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.36.self_attn.v_proj.weight          -> blk.36.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.36.self_attn.o_proj.weight          -> blk.36.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.36.attn_rot_embd\n",
            "model.layers.36.mlp.gate_proj.weight             -> blk.36.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.36.mlp.up_proj.weight               -> blk.36.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.36.mlp.down_proj.weight             -> blk.36.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.36.input_layernorm.weight           -> blk.36.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.36.post_attention_layernorm.weight  -> blk.36.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.37.self_attn.q_proj.weight          -> blk.37.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.37.self_attn.k_proj.weight          -> blk.37.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.37.self_attn.v_proj.weight          -> blk.37.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.37.self_attn.o_proj.weight          -> blk.37.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.37.attn_rot_embd\n",
            "model.layers.37.mlp.gate_proj.weight             -> blk.37.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.37.mlp.up_proj.weight               -> blk.37.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.37.mlp.down_proj.weight             -> blk.37.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.37.input_layernorm.weight           -> blk.37.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.37.post_attention_layernorm.weight  -> blk.37.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.38.self_attn.q_proj.weight          -> blk.38.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.38.self_attn.k_proj.weight          -> blk.38.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.38.self_attn.v_proj.weight          -> blk.38.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.38.self_attn.o_proj.weight          -> blk.38.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.38.attn_rot_embd\n",
            "model.layers.38.mlp.gate_proj.weight             -> blk.38.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.38.mlp.up_proj.weight               -> blk.38.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.38.mlp.down_proj.weight             -> blk.38.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.38.input_layernorm.weight           -> blk.38.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.38.post_attention_layernorm.weight  -> blk.38.ffn_norm.weight                   | F16    | [5120]\n",
            "model.layers.39.self_attn.q_proj.weight          -> blk.39.attn_q.weight                     | F16    | [5120, 5120]\n",
            "model.layers.39.self_attn.k_proj.weight          -> blk.39.attn_k.weight                     | F16    | [5120, 5120]\n",
            "model.layers.39.self_attn.v_proj.weight          -> blk.39.attn_v.weight                     | F16    | [5120, 5120]\n",
            "model.layers.39.self_attn.o_proj.weight          -> blk.39.attn_output.weight                | F16    | [5120, 5120]\n",
            "skipping tensor blk.39.attn_rot_embd\n",
            "model.layers.39.mlp.gate_proj.weight             -> blk.39.ffn_gate.weight                   | F16    | [13824, 5120]\n",
            "model.layers.39.mlp.up_proj.weight               -> blk.39.ffn_up.weight                     | F16    | [13824, 5120]\n",
            "model.layers.39.mlp.down_proj.weight             -> blk.39.ffn_down.weight                   | F16    | [5120, 13824]\n",
            "model.layers.39.input_layernorm.weight           -> blk.39.attn_norm.weight                  | F16    | [5120]\n",
            "model.layers.39.post_attention_layernorm.weight  -> blk.39.ffn_norm.weight                   | F16    | [5120]\n",
            "model.norm.weight                                -> output_norm.weight                       | F16    | [5120]\n",
            "lm_head.weight                                   -> output.weight                            | F16    | [32000, 5120]\n",
            "Writing vicuna-13b-v1.5.gguf, format 7\n",
            "This gguf file is for Little Endian only\n",
            "gguf: Setting special token type bos to 1\n",
            "gguf: Setting special token type eos to 2\n",
            "gguf: Setting special token type pad to 0\n",
            "[  1/363] Writing tensor token_embd.weight                      | size  32000 x   5120  | type Q8_0 | T+  26\n",
            "[  2/363] Writing tensor blk.0.attn_q.weight                    | size   5120 x   5120  | type Q8_0 | T+  27\n",
            "[  3/363] Writing tensor blk.0.attn_k.weight                    | size   5120 x   5120  | type Q8_0 | T+  27\n",
            "[  4/363] Writing tensor blk.0.attn_v.weight                    | size   5120 x   5120  | type Q8_0 | T+  27\n",
            "[  5/363] Writing tensor blk.0.attn_output.weight               | size   5120 x   5120  | type Q8_0 | T+  27\n",
            "[  6/363] Writing tensor blk.0.ffn_gate.weight                  | size  13824 x   5120  | type Q8_0 | T+  28\n",
            "[  7/363] Writing tensor blk.0.ffn_up.weight                    | size  13824 x   5120  | type Q8_0 | T+  28\n",
            "[  8/363] Writing tensor blk.0.ffn_down.weight                  | size   5120 x  13824  | type Q8_0 | T+  29\n",
            "[  9/363] Writing tensor blk.0.attn_norm.weight                 | size   5120           | type F32  | T+  30\n",
            "[ 10/363] Writing tensor blk.0.ffn_norm.weight                  | size   5120           | type F32  | T+  30\n",
            "[ 11/363] Writing tensor blk.1.attn_q.weight                    | size   5120 x   5120  | type Q8_0 | T+  30\n",
            "[ 12/363] Writing tensor blk.1.attn_k.weight                    | size   5120 x   5120  | type Q8_0 | T+  32\n",
            "[ 13/363] Writing tensor blk.1.attn_v.weight                    | size   5120 x   5120  | type Q8_0 | T+  32\n",
            "[ 14/363] Writing tensor blk.1.attn_output.weight               | size   5120 x   5120  | type Q8_0 | T+  32\n",
            "[ 15/363] Writing tensor blk.1.ffn_gate.weight                  | size  13824 x   5120  | type Q8_0 | T+  41\n",
            "[ 16/363] Writing tensor blk.1.ffn_up.weight                    | size  13824 x   5120  | type Q8_0 | T+  43\n",
            "[ 17/363] Writing tensor blk.1.ffn_down.weight                  | size   5120 x  13824  | type Q8_0 | T+  44\n",
            "[ 18/363] Writing tensor blk.1.attn_norm.weight                 | size   5120           | type F32  | T+  44\n",
            "[ 19/363] Writing tensor blk.1.ffn_norm.weight                  | size   5120           | type F32  | T+  44\n",
            "[ 20/363] Writing tensor blk.2.attn_q.weight                    | size   5120 x   5120  | type Q8_0 | T+  44\n",
            "[ 21/363] Writing tensor blk.2.attn_k.weight                    | size   5120 x   5120  | type Q8_0 | T+  44\n",
            "[ 22/363] Writing tensor blk.2.attn_v.weight                    | size   5120 x   5120  | type Q8_0 | T+  45\n",
            "[ 23/363] Writing tensor blk.2.attn_output.weight               | size   5120 x   5120  | type Q8_0 | T+  45\n",
            "[ 24/363] Writing tensor blk.2.ffn_gate.weight                  | size  13824 x   5120  | type Q8_0 | T+  52\n",
            "[ 25/363] Writing tensor blk.2.ffn_up.weight                    | size  13824 x   5120  | type Q8_0 | T+  55\n",
            "[ 26/363] Writing tensor blk.2.ffn_down.weight                  | size   5120 x  13824  | type Q8_0 | T+  57\n",
            "[ 27/363] Writing tensor blk.2.attn_norm.weight                 | size   5120           | type F32  | T+  58\n",
            "[ 28/363] Writing tensor blk.2.ffn_norm.weight                  | size   5120           | type F32  | T+  58\n",
            "[ 29/363] Writing tensor blk.3.attn_q.weight                    | size   5120 x   5120  | type Q8_0 | T+  58\n",
            "[ 30/363] Writing tensor blk.3.attn_k.weight                    | size   5120 x   5120  | type Q8_0 | T+  58\n",
            "[ 31/363] Writing tensor blk.3.attn_v.weight                    | size   5120 x   5120  | type Q8_0 | T+  58\n",
            "[ 32/363] Writing tensor blk.3.attn_output.weight               | size   5120 x   5120  | type Q8_0 | T+  58\n",
            "[ 33/363] Writing tensor blk.3.ffn_gate.weight                  | size  13824 x   5120  | type Q8_0 | T+  63\n",
            "[ 34/363] Writing tensor blk.3.ffn_up.weight                    | size  13824 x   5120  | type Q8_0 | T+  66\n",
            "[ 35/363] Writing tensor blk.3.ffn_down.weight                  | size   5120 x  13824  | type Q8_0 | T+  69\n",
            "[ 36/363] Writing tensor blk.3.attn_norm.weight                 | size   5120           | type F32  | T+  70\n",
            "[ 37/363] Writing tensor blk.3.ffn_norm.weight                  | size   5120           | type F32  | T+  70\n",
            "[ 38/363] Writing tensor blk.4.attn_q.weight                    | size   5120 x   5120  | type Q8_0 | T+  70\n",
            "[ 39/363] Writing tensor blk.4.attn_k.weight                    | size   5120 x   5120  | type Q8_0 | T+  70\n",
            "[ 40/363] Writing tensor blk.4.attn_v.weight                    | size   5120 x   5120  | type Q8_0 | T+  70\n",
            "[ 41/363] Writing tensor blk.4.attn_output.weight               | size   5120 x   5120  | type Q8_0 | T+  71\n",
            "[ 42/363] Writing tensor blk.4.ffn_gate.weight                  | size  13824 x   5120  | type Q8_0 | T+  76\n",
            "[ 43/363] Writing tensor blk.4.ffn_up.weight                    | size  13824 x   5120  | type Q8_0 | T+  79\n",
            "[ 44/363] Writing tensor blk.4.ffn_down.weight                  | size   5120 x  13824  | type Q8_0 | T+  80\n",
            "[ 45/363] Writing tensor blk.4.attn_norm.weight                 | size   5120           | type F32  | T+  81\n",
            "[ 46/363] Writing tensor blk.4.ffn_norm.weight                  | size   5120           | type F32  | T+  81\n",
            "[ 47/363] Writing tensor blk.5.attn_q.weight                    | size   5120 x   5120  | type Q8_0 | T+  81\n",
            "[ 48/363] Writing tensor blk.5.attn_k.weight                    | size   5120 x   5120  | type Q8_0 | T+  81\n",
            "[ 49/363] Writing tensor blk.5.attn_v.weight                    | size   5120 x   5120  | type Q8_0 | T+  81\n",
            "[ 50/363] Writing tensor blk.5.attn_output.weight               | size   5120 x   5120  | type Q8_0 | T+  81\n",
            "[ 51/363] Writing tensor blk.5.ffn_gate.weight                  | size  13824 x   5120  | type Q8_0 | T+  90\n",
            "[ 52/363] Writing tensor blk.5.ffn_up.weight                    | size  13824 x   5120  | type Q8_0 | T+  91\n",
            "[ 53/363] Writing tensor blk.5.ffn_down.weight                  | size   5120 x  13824  | type Q8_0 | T+  93\n",
            "[ 54/363] Writing tensor blk.5.attn_norm.weight                 | size   5120           | type F32  | T+  94\n",
            "[ 55/363] Writing tensor blk.5.ffn_norm.weight                  | size   5120           | type F32  | T+  94\n",
            "[ 56/363] Writing tensor blk.6.attn_q.weight                    | size   5120 x   5120  | type Q8_0 | T+  94\n",
            "[ 57/363] Writing tensor blk.6.attn_k.weight                    | size   5120 x   5120  | type Q8_0 | T+  94\n",
            "[ 58/363] Writing tensor blk.6.attn_v.weight                    | size   5120 x   5120  | type Q8_0 | T+  94\n",
            "[ 59/363] Writing tensor blk.6.attn_output.weight               | size   5120 x   5120  | type Q8_0 | T+  94\n",
            "[ 60/363] Writing tensor blk.6.ffn_gate.weight                  | size  13824 x   5120  | type Q8_0 | T+ 100\n",
            "[ 61/363] Writing tensor blk.6.ffn_up.weight                    | size  13824 x   5120  | type Q8_0 | T+ 105\n",
            "[ 62/363] Writing tensor blk.6.ffn_down.weight                  | size   5120 x  13824  | type Q8_0 | T+ 105\n",
            "[ 63/363] Writing tensor blk.6.attn_norm.weight                 | size   5120           | type F32  | T+ 106\n",
            "[ 64/363] Writing tensor blk.6.ffn_norm.weight                  | size   5120           | type F32  | T+ 106\n",
            "[ 65/363] Writing tensor blk.7.attn_q.weight                    | size   5120 x   5120  | type Q8_0 | T+ 106\n",
            "[ 66/363] Writing tensor blk.7.attn_k.weight                    | size   5120 x   5120  | type Q8_0 | T+ 106\n",
            "[ 67/363] Writing tensor blk.7.attn_v.weight                    | size   5120 x   5120  | type Q8_0 | T+ 106\n",
            "[ 68/363] Writing tensor blk.7.attn_output.weight               | size   5120 x   5120  | type Q8_0 | T+ 106\n",
            "[ 69/363] Writing tensor blk.7.ffn_gate.weight                  | size  13824 x   5120  | type Q8_0 | T+ 113\n",
            "[ 70/363] Writing tensor blk.7.ffn_up.weight                    | size  13824 x   5120  | type Q8_0 | T+ 117\n",
            "[ 71/363] Writing tensor blk.7.ffn_down.weight                  | size   5120 x  13824  | type Q8_0 | T+ 119\n",
            "[ 72/363] Writing tensor blk.7.attn_norm.weight                 | size   5120           | type F32  | T+ 119\n",
            "[ 73/363] Writing tensor blk.7.ffn_norm.weight                  | size   5120           | type F32  | T+ 119\n",
            "[ 74/363] Writing tensor blk.8.attn_q.weight                    | size   5120 x   5120  | type Q8_0 | T+ 119\n",
            "[ 75/363] Writing tensor blk.8.attn_k.weight                    | size   5120 x   5120  | type Q8_0 | T+ 119\n",
            "[ 76/363] Writing tensor blk.8.attn_v.weight                    | size   5120 x   5120  | type Q8_0 | T+ 120\n",
            "[ 77/363] Writing tensor blk.8.attn_output.weight               | size   5120 x   5120  | type Q8_0 | T+ 120\n",
            "[ 78/363] Writing tensor blk.8.ffn_gate.weight                  | size  13824 x   5120  | type Q8_0 | T+ 126\n",
            "[ 79/363] Writing tensor blk.8.ffn_up.weight                    | size  13824 x   5120  | type Q8_0 | T+ 129\n",
            "[ 80/363] Writing tensor blk.8.ffn_down.weight                  | size   5120 x  13824  | type Q8_0 | T+ 132\n",
            "[ 81/363] Writing tensor blk.8.attn_norm.weight                 | size   5120           | type F32  | T+ 132\n",
            "[ 82/363] Writing tensor blk.8.ffn_norm.weight                  | size   5120           | type F32  | T+ 132\n",
            "[ 83/363] Writing tensor blk.9.attn_q.weight                    | size   5120 x   5120  | type Q8_0 | T+ 132\n",
            "[ 84/363] Writing tensor blk.9.attn_k.weight                    | size   5120 x   5120  | type Q8_0 | T+ 132\n",
            "[ 85/363] Writing tensor blk.9.attn_v.weight                    | size   5120 x   5120  | type Q8_0 | T+ 133\n",
            "[ 86/363] Writing tensor blk.9.attn_output.weight               | size   5120 x   5120  | type Q8_0 | T+ 133\n",
            "[ 87/363] Writing tensor blk.9.ffn_gate.weight                  | size  13824 x   5120  | type Q8_0 | T+ 138\n",
            "[ 88/363] Writing tensor blk.9.ffn_up.weight                    | size  13824 x   5120  | type Q8_0 | T+ 142\n",
            "[ 89/363] Writing tensor blk.9.ffn_down.weight                  | size   5120 x  13824  | type Q8_0 | T+ 145\n",
            "[ 90/363] Writing tensor blk.9.attn_norm.weight                 | size   5120           | type F32  | T+ 146\n",
            "[ 91/363] Writing tensor blk.9.ffn_norm.weight                  | size   5120           | type F32  | T+ 146\n",
            "[ 92/363] Writing tensor blk.10.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 146\n",
            "[ 93/363] Writing tensor blk.10.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 146\n",
            "[ 94/363] Writing tensor blk.10.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 146\n",
            "[ 95/363] Writing tensor blk.10.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 146\n",
            "[ 96/363] Writing tensor blk.10.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 151\n",
            "[ 97/363] Writing tensor blk.10.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 155\n",
            "[ 98/363] Writing tensor blk.10.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 156\n",
            "[ 99/363] Writing tensor blk.10.attn_norm.weight                | size   5120           | type F32  | T+ 156\n",
            "[100/363] Writing tensor blk.10.ffn_norm.weight                 | size   5120           | type F32  | T+ 156\n",
            "[101/363] Writing tensor blk.11.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 156\n",
            "[102/363] Writing tensor blk.11.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 157\n",
            "[103/363] Writing tensor blk.11.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 157\n",
            "[104/363] Writing tensor blk.11.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 158\n",
            "[105/363] Writing tensor blk.11.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 167\n",
            "[106/363] Writing tensor blk.11.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 170\n",
            "[107/363] Writing tensor blk.11.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 171\n",
            "[108/363] Writing tensor blk.11.attn_norm.weight                | size   5120           | type F32  | T+ 171\n",
            "[109/363] Writing tensor blk.11.ffn_norm.weight                 | size   5120           | type F32  | T+ 171\n",
            "[110/363] Writing tensor blk.12.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 171\n",
            "[111/363] Writing tensor blk.12.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 171\n",
            "[112/363] Writing tensor blk.12.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 172\n",
            "[113/363] Writing tensor blk.12.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 172\n",
            "[114/363] Writing tensor blk.12.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 180\n",
            "[115/363] Writing tensor blk.12.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 182\n",
            "[116/363] Writing tensor blk.12.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 183\n",
            "[117/363] Writing tensor blk.12.attn_norm.weight                | size   5120           | type F32  | T+ 183\n",
            "[118/363] Writing tensor blk.12.ffn_norm.weight                 | size   5120           | type F32  | T+ 183\n",
            "[119/363] Writing tensor blk.13.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 183\n",
            "[120/363] Writing tensor blk.13.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 183\n",
            "[121/363] Writing tensor blk.13.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 184\n",
            "[122/363] Writing tensor blk.13.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 184\n",
            "[123/363] Writing tensor blk.13.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 193\n",
            "[124/363] Writing tensor blk.13.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 194\n",
            "[125/363] Writing tensor blk.13.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 196\n",
            "[126/363] Writing tensor blk.13.attn_norm.weight                | size   5120           | type F32  | T+ 196\n",
            "[127/363] Writing tensor blk.13.ffn_norm.weight                 | size   5120           | type F32  | T+ 196\n",
            "[128/363] Writing tensor blk.14.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 196\n",
            "[129/363] Writing tensor blk.14.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 196\n",
            "[130/363] Writing tensor blk.14.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 198\n",
            "[131/363] Writing tensor blk.14.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 199\n",
            "[132/363] Writing tensor blk.14.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 203\n",
            "[133/363] Writing tensor blk.14.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 207\n",
            "[134/363] Writing tensor blk.14.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 209\n",
            "[135/363] Writing tensor blk.14.attn_norm.weight                | size   5120           | type F32  | T+ 209\n",
            "[136/363] Writing tensor blk.14.ffn_norm.weight                 | size   5120           | type F32  | T+ 209\n",
            "[137/363] Writing tensor blk.15.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 209\n",
            "[138/363] Writing tensor blk.15.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 209\n",
            "[139/363] Writing tensor blk.15.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 210\n",
            "[140/363] Writing tensor blk.15.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 210\n",
            "[141/363] Writing tensor blk.15.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 217\n",
            "[142/363] Writing tensor blk.15.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 220\n",
            "[143/363] Writing tensor blk.15.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 222\n",
            "[144/363] Writing tensor blk.15.attn_norm.weight                | size   5120           | type F32  | T+ 223\n",
            "[145/363] Writing tensor blk.15.ffn_norm.weight                 | size   5120           | type F32  | T+ 223\n",
            "[146/363] Writing tensor blk.16.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 223\n",
            "[147/363] Writing tensor blk.16.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 223\n",
            "[148/363] Writing tensor blk.16.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 223\n",
            "[149/363] Writing tensor blk.16.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 223\n",
            "[150/363] Writing tensor blk.16.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 228\n",
            "[151/363] Writing tensor blk.16.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 232\n",
            "[152/363] Writing tensor blk.16.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 233\n",
            "[153/363] Writing tensor blk.16.attn_norm.weight                | size   5120           | type F32  | T+ 234\n",
            "[154/363] Writing tensor blk.16.ffn_norm.weight                 | size   5120           | type F32  | T+ 234\n",
            "[155/363] Writing tensor blk.17.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 234\n",
            "[156/363] Writing tensor blk.17.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 235\n",
            "[157/363] Writing tensor blk.17.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 235\n",
            "[158/363] Writing tensor blk.17.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 235\n",
            "[159/363] Writing tensor blk.17.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 241\n",
            "[160/363] Writing tensor blk.17.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 244\n",
            "[161/363] Writing tensor blk.17.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 245\n",
            "[162/363] Writing tensor blk.17.attn_norm.weight                | size   5120           | type F32  | T+ 245\n",
            "[163/363] Writing tensor blk.17.ffn_norm.weight                 | size   5120           | type F32  | T+ 246\n",
            "[164/363] Writing tensor blk.18.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 246\n",
            "[165/363] Writing tensor blk.18.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 246\n",
            "[166/363] Writing tensor blk.18.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 246\n",
            "[167/363] Writing tensor blk.18.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 246\n",
            "[168/363] Writing tensor blk.18.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 254\n",
            "[169/363] Writing tensor blk.18.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 257\n",
            "[170/363] Writing tensor blk.18.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 258\n",
            "[171/363] Writing tensor blk.18.attn_norm.weight                | size   5120           | type F32  | T+ 258\n",
            "[172/363] Writing tensor blk.18.ffn_norm.weight                 | size   5120           | type F32  | T+ 258\n",
            "[173/363] Writing tensor blk.19.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 258\n",
            "[174/363] Writing tensor blk.19.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 258\n",
            "[175/363] Writing tensor blk.19.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 258\n",
            "[176/363] Writing tensor blk.19.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 259\n",
            "[177/363] Writing tensor blk.19.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 267\n",
            "[178/363] Writing tensor blk.19.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 269\n",
            "[179/363] Writing tensor blk.19.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 271\n",
            "[180/363] Writing tensor blk.19.attn_norm.weight                | size   5120           | type F32  | T+ 272\n",
            "[181/363] Writing tensor blk.19.ffn_norm.weight                 | size   5120           | type F32  | T+ 272\n",
            "[182/363] Writing tensor blk.20.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 272\n",
            "[183/363] Writing tensor blk.20.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 272\n",
            "[184/363] Writing tensor blk.20.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 272\n",
            "[185/363] Writing tensor blk.20.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 272\n",
            "[186/363] Writing tensor blk.20.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 277\n",
            "[187/363] Writing tensor blk.20.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 282\n",
            "[188/363] Writing tensor blk.20.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 283\n",
            "[189/363] Writing tensor blk.20.attn_norm.weight                | size   5120           | type F32  | T+ 284\n",
            "[190/363] Writing tensor blk.20.ffn_norm.weight                 | size   5120           | type F32  | T+ 284\n",
            "[191/363] Writing tensor blk.21.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 284\n",
            "[192/363] Writing tensor blk.21.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 284\n",
            "[193/363] Writing tensor blk.21.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 284\n",
            "[194/363] Writing tensor blk.21.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 284\n",
            "[195/363] Writing tensor blk.21.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 291\n",
            "[196/363] Writing tensor blk.21.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 294\n",
            "[197/363] Writing tensor blk.21.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 296\n",
            "[198/363] Writing tensor blk.21.attn_norm.weight                | size   5120           | type F32  | T+ 297\n",
            "[199/363] Writing tensor blk.21.ffn_norm.weight                 | size   5120           | type F32  | T+ 297\n",
            "[200/363] Writing tensor blk.22.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 297\n",
            "[201/363] Writing tensor blk.22.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 297\n",
            "[202/363] Writing tensor blk.22.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 297\n",
            "[203/363] Writing tensor blk.22.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 298\n",
            "[204/363] Writing tensor blk.22.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 304\n",
            "[205/363] Writing tensor blk.22.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 306\n",
            "[206/363] Writing tensor blk.22.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 308\n",
            "[207/363] Writing tensor blk.22.attn_norm.weight                | size   5120           | type F32  | T+ 308\n",
            "[208/363] Writing tensor blk.22.ffn_norm.weight                 | size   5120           | type F32  | T+ 308\n",
            "[209/363] Writing tensor blk.23.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 308\n",
            "[210/363] Writing tensor blk.23.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 308\n",
            "[211/363] Writing tensor blk.23.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 309\n",
            "[212/363] Writing tensor blk.23.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 309\n",
            "[213/363] Writing tensor blk.23.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 317\n",
            "[214/363] Writing tensor blk.23.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 319\n",
            "[215/363] Writing tensor blk.23.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 320\n",
            "[216/363] Writing tensor blk.23.attn_norm.weight                | size   5120           | type F32  | T+ 321\n",
            "[217/363] Writing tensor blk.23.ffn_norm.weight                 | size   5120           | type F32  | T+ 321\n",
            "[218/363] Writing tensor blk.24.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 321\n",
            "[219/363] Writing tensor blk.24.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 321\n",
            "[220/363] Writing tensor blk.24.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 321\n",
            "[221/363] Writing tensor blk.24.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 321\n",
            "[222/363] Writing tensor blk.24.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 329\n",
            "[223/363] Writing tensor blk.24.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 332\n",
            "[224/363] Writing tensor blk.24.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 333\n",
            "[225/363] Writing tensor blk.24.attn_norm.weight                | size   5120           | type F32  | T+ 334\n",
            "[226/363] Writing tensor blk.24.ffn_norm.weight                 | size   5120           | type F32  | T+ 334\n",
            "[227/363] Writing tensor blk.25.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 334\n",
            "[228/363] Writing tensor blk.25.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 334\n",
            "[229/363] Writing tensor blk.25.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 334\n",
            "[230/363] Writing tensor blk.25.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 334\n",
            "[231/363] Writing tensor blk.25.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 340\n",
            "[232/363] Writing tensor blk.25.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 344\n",
            "[233/363] Writing tensor blk.25.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 345\n",
            "[234/363] Writing tensor blk.25.attn_norm.weight                | size   5120           | type F32  | T+ 346\n",
            "[235/363] Writing tensor blk.25.ffn_norm.weight                 | size   5120           | type F32  | T+ 346\n",
            "[236/363] Writing tensor blk.26.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 346\n",
            "[237/363] Writing tensor blk.26.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 346\n",
            "[238/363] Writing tensor blk.26.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 346\n",
            "[239/363] Writing tensor blk.26.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 346\n",
            "[240/363] Writing tensor blk.26.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 352\n",
            "[241/363] Writing tensor blk.26.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 357\n",
            "[242/363] Writing tensor blk.26.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 358\n",
            "[243/363] Writing tensor blk.26.attn_norm.weight                | size   5120           | type F32  | T+ 359\n",
            "[244/363] Writing tensor blk.26.ffn_norm.weight                 | size   5120           | type F32  | T+ 359\n",
            "[245/363] Writing tensor blk.27.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 359\n",
            "[246/363] Writing tensor blk.27.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 359\n",
            "[247/363] Writing tensor blk.27.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 362\n",
            "[248/363] Writing tensor blk.27.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 363\n",
            "[249/363] Writing tensor blk.27.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 366\n",
            "[250/363] Writing tensor blk.27.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 368\n",
            "[251/363] Writing tensor blk.27.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 372\n",
            "[252/363] Writing tensor blk.27.attn_norm.weight                | size   5120           | type F32  | T+ 373\n",
            "[253/363] Writing tensor blk.27.ffn_norm.weight                 | size   5120           | type F32  | T+ 373\n",
            "[254/363] Writing tensor blk.28.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 373\n",
            "[255/363] Writing tensor blk.28.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 373\n",
            "[256/363] Writing tensor blk.28.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 373\n",
            "[257/363] Writing tensor blk.28.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 373\n",
            "[258/363] Writing tensor blk.28.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 375\n",
            "[259/363] Writing tensor blk.28.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 383\n",
            "[260/363] Writing tensor blk.28.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 386\n",
            "[261/363] Writing tensor blk.28.attn_norm.weight                | size   5120           | type F32  | T+ 387\n",
            "[262/363] Writing tensor blk.28.ffn_norm.weight                 | size   5120           | type F32  | T+ 387\n",
            "[263/363] Writing tensor blk.29.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 387\n",
            "[264/363] Writing tensor blk.29.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 387\n",
            "[265/363] Writing tensor blk.29.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 387\n",
            "[266/363] Writing tensor blk.29.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 387\n",
            "[267/363] Writing tensor blk.29.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 392\n",
            "[268/363] Writing tensor blk.29.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 396\n",
            "[269/363] Writing tensor blk.29.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 397\n",
            "[270/363] Writing tensor blk.29.attn_norm.weight                | size   5120           | type F32  | T+ 398\n",
            "[271/363] Writing tensor blk.29.ffn_norm.weight                 | size   5120           | type F32  | T+ 398\n",
            "[272/363] Writing tensor blk.30.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 398\n",
            "[273/363] Writing tensor blk.30.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 398\n",
            "[274/363] Writing tensor blk.30.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 398\n",
            "[275/363] Writing tensor blk.30.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 399\n",
            "[276/363] Writing tensor blk.30.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 406\n",
            "[277/363] Writing tensor blk.30.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 408\n",
            "[278/363] Writing tensor blk.30.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 409\n",
            "[279/363] Writing tensor blk.30.attn_norm.weight                | size   5120           | type F32  | T+ 410\n",
            "[280/363] Writing tensor blk.30.ffn_norm.weight                 | size   5120           | type F32  | T+ 410\n",
            "[281/363] Writing tensor blk.31.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 410\n",
            "[282/363] Writing tensor blk.31.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 410\n",
            "[283/363] Writing tensor blk.31.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 410\n",
            "[284/363] Writing tensor blk.31.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 410\n",
            "[285/363] Writing tensor blk.31.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 418\n",
            "[286/363] Writing tensor blk.31.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 421\n",
            "[287/363] Writing tensor blk.31.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 422\n",
            "[288/363] Writing tensor blk.31.attn_norm.weight                | size   5120           | type F32  | T+ 423\n",
            "[289/363] Writing tensor blk.31.ffn_norm.weight                 | size   5120           | type F32  | T+ 423\n",
            "[290/363] Writing tensor blk.32.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 423\n",
            "[291/363] Writing tensor blk.32.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 423\n",
            "[292/363] Writing tensor blk.32.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 423\n",
            "[293/363] Writing tensor blk.32.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 424\n",
            "[294/363] Writing tensor blk.32.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 431\n",
            "[295/363] Writing tensor blk.32.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 433\n",
            "[296/363] Writing tensor blk.32.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 435\n",
            "[297/363] Writing tensor blk.32.attn_norm.weight                | size   5120           | type F32  | T+ 436\n",
            "[298/363] Writing tensor blk.32.ffn_norm.weight                 | size   5120           | type F32  | T+ 436\n",
            "[299/363] Writing tensor blk.33.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 436\n",
            "[300/363] Writing tensor blk.33.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 436\n",
            "[301/363] Writing tensor blk.33.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 436\n",
            "[302/363] Writing tensor blk.33.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 436\n",
            "[303/363] Writing tensor blk.33.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 446\n",
            "[304/363] Writing tensor blk.33.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 449\n",
            "[305/363] Writing tensor blk.33.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 451\n",
            "[306/363] Writing tensor blk.33.attn_norm.weight                | size   5120           | type F32  | T+ 452\n",
            "[307/363] Writing tensor blk.33.ffn_norm.weight                 | size   5120           | type F32  | T+ 452\n",
            "[308/363] Writing tensor blk.34.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 452\n",
            "[309/363] Writing tensor blk.34.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 452\n",
            "[310/363] Writing tensor blk.34.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 453\n",
            "[311/363] Writing tensor blk.34.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 453\n",
            "[312/363] Writing tensor blk.34.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 457\n",
            "[313/363] Writing tensor blk.34.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 463\n",
            "[314/363] Writing tensor blk.34.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 465\n",
            "[315/363] Writing tensor blk.34.attn_norm.weight                | size   5120           | type F32  | T+ 465\n",
            "[316/363] Writing tensor blk.34.ffn_norm.weight                 | size   5120           | type F32  | T+ 465\n",
            "[317/363] Writing tensor blk.35.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 465\n",
            "[318/363] Writing tensor blk.35.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 465\n",
            "[319/363] Writing tensor blk.35.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 465\n",
            "[320/363] Writing tensor blk.35.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 465\n",
            "[321/363] Writing tensor blk.35.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 471\n",
            "[322/363] Writing tensor blk.35.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 476\n",
            "[323/363] Writing tensor blk.35.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 477\n",
            "[324/363] Writing tensor blk.35.attn_norm.weight                | size   5120           | type F32  | T+ 478\n",
            "[325/363] Writing tensor blk.35.ffn_norm.weight                 | size   5120           | type F32  | T+ 478\n",
            "[326/363] Writing tensor blk.36.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 478\n",
            "[327/363] Writing tensor blk.36.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 479\n",
            "[328/363] Writing tensor blk.36.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 479\n",
            "[329/363] Writing tensor blk.36.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 479\n",
            "[330/363] Writing tensor blk.36.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 485\n",
            "[331/363] Writing tensor blk.36.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 488\n",
            "[332/363] Writing tensor blk.36.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 489\n",
            "[333/363] Writing tensor blk.36.attn_norm.weight                | size   5120           | type F32  | T+ 490\n",
            "[334/363] Writing tensor blk.36.ffn_norm.weight                 | size   5120           | type F32  | T+ 490\n",
            "[335/363] Writing tensor blk.37.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 490\n",
            "[336/363] Writing tensor blk.37.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 490\n",
            "[337/363] Writing tensor blk.37.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 490\n",
            "[338/363] Writing tensor blk.37.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 491\n",
            "[339/363] Writing tensor blk.37.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 498\n",
            "[340/363] Writing tensor blk.37.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 501\n",
            "[341/363] Writing tensor blk.37.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 502\n",
            "[342/363] Writing tensor blk.37.attn_norm.weight                | size   5120           | type F32  | T+ 502\n",
            "[343/363] Writing tensor blk.37.ffn_norm.weight                 | size   5120           | type F32  | T+ 502\n",
            "[344/363] Writing tensor blk.38.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 502\n",
            "[345/363] Writing tensor blk.38.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 502\n",
            "[346/363] Writing tensor blk.38.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 502\n",
            "[347/363] Writing tensor blk.38.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 503\n",
            "[348/363] Writing tensor blk.38.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 511\n",
            "[349/363] Writing tensor blk.38.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 514\n",
            "[350/363] Writing tensor blk.38.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 515\n",
            "[351/363] Writing tensor blk.38.attn_norm.weight                | size   5120           | type F32  | T+ 515\n",
            "[352/363] Writing tensor blk.38.ffn_norm.weight                 | size   5120           | type F32  | T+ 515\n",
            "[353/363] Writing tensor blk.39.attn_q.weight                   | size   5120 x   5120  | type Q8_0 | T+ 515\n",
            "[354/363] Writing tensor blk.39.attn_k.weight                   | size   5120 x   5120  | type Q8_0 | T+ 516\n",
            "[355/363] Writing tensor blk.39.attn_v.weight                   | size   5120 x   5120  | type Q8_0 | T+ 516\n",
            "[356/363] Writing tensor blk.39.attn_output.weight              | size   5120 x   5120  | type Q8_0 | T+ 516\n",
            "[357/363] Writing tensor blk.39.ffn_gate.weight                 | size  13824 x   5120  | type Q8_0 | T+ 520\n",
            "[358/363] Writing tensor blk.39.ffn_up.weight                   | size  13824 x   5120  | type Q8_0 | T+ 522\n",
            "[359/363] Writing tensor blk.39.ffn_down.weight                 | size   5120 x  13824  | type Q8_0 | T+ 524\n",
            "[360/363] Writing tensor blk.39.attn_norm.weight                | size   5120           | type F32  | T+ 524\n",
            "[361/363] Writing tensor blk.39.ffn_norm.weight                 | size   5120           | type F32  | T+ 524\n",
            "[362/363] Writing tensor output_norm.weight                     | size   5120           | type F32  | T+ 524\n",
            "[363/363] Writing tensor output.weight                          | size  32000 x   5120  | type Q8_0 | T+ 527\n",
            "Wrote vicuna-13b-v1.5.gguf\n"
          ]
        }
      ],
      "source": [
        "!python /content/convert.py /content/vicuna-hf/ --outfile vicuna-13b-v1.5.gguf --outtype q8_0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLC Chat Application"
      ],
      "metadata": {
        "id": "-Or9v_Qdw05e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69ZXZv1vmonT",
        "outputId": "be1e8dd4-a8f6-4f05-aafb-1d14df3114e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://mlc.ai/wheels\n",
            "Collecting mlc-chat-nightly\n",
            "  Downloading https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_chat_nightly-0.1.dev529-cp310-cp310-manylinux_2_28_x86_64.whl (21.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.9/21.9 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mlc-ai-nightly\n",
            "  Downloading https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_ai_nightly-0.12.dev1690-cp310-cp310-manylinux_2_28_x86_64.whl (51.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi (from mlc-chat-nightly)\n",
            "  Downloading fastapi-0.104.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn (from mlc-chat-nightly)\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shortuuid (from mlc-chat-nightly)\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly) (23.1.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly) (2.2.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly) (4.4.2)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly) (0.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly) (1.23.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly) (5.9.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly) (1.11.3)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly) (6.3.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly) (4.5.0)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->mlc-chat-nightly) (3.7.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi->mlc-chat-nightly) (1.10.13)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->mlc-chat-nightly)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions (from mlc-ai-nightly)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->mlc-chat-nightly) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn->mlc-chat-nightly)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->mlc-chat-nightly) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->mlc-chat-nightly) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->mlc-chat-nightly) (1.1.3)\n",
            "Installing collected packages: typing-extensions, shortuuid, h11, uvicorn, starlette, mlc-ai-nightly, fastapi, mlc-chat-nightly\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fastapi-0.104.0 h11-0.14.0 mlc-ai-nightly-0.12.dev1690 mlc-chat-nightly-0.1.dev529 shortuuid-1.0.11 starlette-0.27.0 typing-extensions-4.8.0 uvicorn-0.23.2\n"
          ]
        }
      ],
      "source": [
        "!pip install --pre -U -f https://mlc.ai/wheels mlc-chat-nightly mlc-ai-nightly\n",
        "!git lfs install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p dist/prebuilt\n",
        "!git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git dist/prebuilt/lib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTKFNuhavhc5",
        "outputId": "b9186101-72bc-408f-9c0f-c8f299886b4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dist/prebuilt/lib'...\n",
            "remote: Enumerating objects: 328, done.\u001b[K\n",
            "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 328 (delta 56), reused 54 (delta 54), pack-reused 268\u001b[K\n",
            "Receiving objects: 100% (328/328), 118.28 MiB | 28.84 MiB/s, done.\n",
            "Resolving deltas: 100% (237/237), done.\n",
            "Updating files: 100% (77/77), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd dist/prebuilt && git clone https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q4f16_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTMGC9ESvlLy",
        "outputId": "9be4eb47-9e4e-4b4e-b6fb-88f87bd4564a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mlc-chat-Llama-2-7b-chat-hf-q4f16_1'...\n",
            "remote: Enumerating objects: 129, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 129 (delta 0), reused 0 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (129/129), 500.53 KiB | 12.21 MiB/s, done.\n",
            "Filtering content: 100% (116/116), 3.53 GiB | 47.78 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mlc_chat import ChatModule\n",
        "from mlc_chat.callback import StreamToStdout"
      ],
      "metadata": {
        "id": "iJ9tTpufwqUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4LfFbBhmzk3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 735
        },
        "outputId": "c4afdaa8-7cb7-42e7-8afd-c0eaaa4d9b8a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "InternalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-bde192190a4e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"vicuna-13b-v1.5.gguf\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mlc_chat/chat_module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, device, chat_config, model_lib_path)\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopencl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_detect_local_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"System automatically detected device: {device_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mlc_chat/chat_module.py\u001b[0m in \u001b[0;36m_detect_local_device\u001b[0;34m(device_id)\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvulkan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvulkan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vulkan\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopencl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tvm/_ffi/runtime_ctypes.py\u001b[0m in \u001b[0;36mexist\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \"\"\"\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_GetDeviceAttr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tvm/_ffi/runtime_ctypes.py\u001b[0m in \u001b[0;36m_GetDeviceAttr\u001b[0;34m(self, device_type, device_id, attr_id)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mruntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ffi_api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mruntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ffi_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetDeviceAttr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mtvm/_ffi/_cython/./packed_func.pxi\u001b[0m in \u001b[0;36mtvm._ffi._cy3.core.PackedFuncBase.__call__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mtvm/_ffi/_cython/./packed_func.pxi\u001b[0m in \u001b[0;36mtvm._ffi._cy3.core.FuncCall\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mtvm/_ffi/_cython/./packed_func.pxi\u001b[0m in \u001b[0;36mtvm._ffi._cy3.core.FuncCall3\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mtvm/_ffi/_cython/./base.pxi\u001b[0m in \u001b[0;36mtvm._ffi._cy3.core.CHECK_CALL\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tvm/_ffi/base.py\u001b[0m in \u001b[0;36mraise_last_ffi_error\u001b[0;34m()\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTVMDropLastPythonError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mpy_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInternalError\u001b[0m: Traceback (most recent call last):\n  7: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<__mk_TVM1::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, __mk_TVM1, tvm::runtime::TVMRetValue)\n  6: tvm::runtime::DeviceAPIManager::GetAPI(int, bool)\n  5: tvm::runtime::DeviceAPIManager::GetAPI(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, bool) [clone .isra.0]\n  4: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::vulkan::__mk_TVM0::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::vulkan::__mk_TVM0, tvm::runtime::TVMRetValue)\n  3: tvm::runtime::vulkan::VulkanDeviceAPI::Global()\n  2: tvm::runtime::vulkan::VulkanDeviceAPI::VulkanDeviceAPI()\n  1: tvm::runtime::vulkan::VulkanInstance::VulkanInstance()\n  0: _ZN3tvm7runtime6deta\n  File \"/workspace/tvm/src/runtime/vulkan/vulkan_instance.cc\", line 111\nInternalError: Check failed: (__e == VK_SUCCESS) is false: Vulkan Error, code=-9: VK_ERROR_INCOMPATIBLE_DRIVER"
          ]
        }
      ],
      "source": [
        "model = \"vicuna-13b-v1.5.gguf\"\n",
        "cm = ChatModule(model=model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = cm.generate(\n",
        "    prompt=\"When will we achieve Artificial General Intelligence?\",\n",
        "    progress_callback=StreamToStdout(callback_interval=2),\n",
        ")"
      ],
      "metadata": {
        "id": "udxRzxrXqi1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gLCph73ZvsuQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}